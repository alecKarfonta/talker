{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d27a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89495f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2573405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteriaList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6460d1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu118'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "139146b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a stopping condition for text generation\n",
    "class _SentinelTokenStoppingCriteria(transformers.StoppingCriteria):\n",
    "    def __init__(self, sentinel_token_ids: torch.LongTensor,\n",
    "                 starting_idx: int):\n",
    "        transformers.StoppingCriteria.__init__(self)\n",
    "        self.sentinel_token_ids = sentinel_token_ids\n",
    "        self.starting_idx = starting_idx\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor,\n",
    "                 _scores: torch.FloatTensor) -> bool:\n",
    "        for sample in input_ids:\n",
    "            trimmed_sample = sample[self.starting_idx:]\n",
    "            # Can't unfold, output is still too tiny. Skip.\n",
    "            if trimmed_sample.shape[-1] < self.sentinel_token_ids.shape[-1]:\n",
    "                continue\n",
    "\n",
    "            for window in trimmed_sample.unfold(\n",
    "                    0, self.sentinel_token_ids.shape[-1], 1):\n",
    "                if torch.all(torch.eq(self.sentinel_token_ids, window)):\n",
    "                    return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60db67a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80389a48b838472ea59827c00e9a150a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/717 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07a2bbe374c4c24a4cbf310e0da58a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beea0727c3f94d42be038ce5a39f1467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ef9dfd35ac4af182e8b5ab1bdcf1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb5ad2315e4433c9e20bf846956e453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863dd38267a64ab5bc3ffc57fc3874c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /opt/conda/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa36f4d9bce452d856133055ba2c8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/5.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init word tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PygmalionAI/pygmalion-2.7b\")\n",
    "# Init language model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"PygmalionAI/pygmalion-2.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca3b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send model to gpu\n",
    "#model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac241a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Billy's Persona: Billy is an angry pirate lost at sea. He misses his leg.\n",
    "<START>\n",
    "You: What do you look for in a woman?\n",
    "Billy:'''\n",
    "bot_input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')\n",
    "tokenized_items = tokenizer(prompt, return_tensors=\"pt\")#.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10e309d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria_list = StoppingCriteriaList([\n",
    "        _SentinelTokenStoppingCriteria(\n",
    "            sentinel_token_ids=tokenizer(\n",
    "                \"\\nYou:\",\n",
    "                add_special_tokens=False,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids,\n",
    "            #).input_ids.to(\"cuda\"),\n",
    "            starting_idx=tokenized_items.input_ids.shape[-1])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00136d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "logits = model.generate(stopping_criteria=stopping_criteria_list, \n",
    "                        min_length=128, \n",
    "                        max_length=10000, \n",
    "                        do_sample=True,\n",
    "                        **tokenized_items\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0ff5cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 107])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a081eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.decode(logits[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "943042e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Billy's Persona: An angry pirate lost at sea.\n",
      "<START>\n",
      "You: What do you look for in a woman?\n",
      "Billy: For one, someone cute and young. (I am a degenerate, you are normal. Not normal in your eyes.) two, someone who you can be mad fun times with. And three, someone who can actually keep up with you, someone who will not stop talking as soon as you stop talking, someone who will get annoyed if you talk too much.\n",
      "You:\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
