{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store\n",
    "\n",
    "!huggingface-cli login --token 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 --add-to-git-credential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_0.gguf?ref=localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv zephyr-7b-beta.Q4_0.gguf?ref=localhost zephyr-7b-beta.Q4_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_tokens_per_second(model, tokenizer, prompts: list, max_tokens: int = 500) -> float:\n",
    "\n",
    "    total_tokens_generated = 0\n",
    "    total_time_taken = 0.0\n",
    "\n",
    "\n",
    "    for prompt in prompts:\n",
    "        # Tokenize the input prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # Measure the time taken to generate the output\n",
    "        start_time = time.time()\n",
    "        output = model.generate(inputs['input_ids'].to(device), max_length=inputs['input_ids'].shape[1] + max_tokens, do_sample=False)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate the number of tokens generated\n",
    "        num_tokens_generated = output.shape[1] - inputs['input_ids'].shape[1]\n",
    "        \n",
    "        # Calculate the time taken\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        # Update the total tokens and time\n",
    "        total_tokens_generated += num_tokens_generated\n",
    "        total_time_taken += time_taken\n",
    "    \n",
    "    \n",
    "    # Calculate average tokens per second\n",
    "    average_tokens_per_second = total_tokens_generated / total_time_taken\n",
    "    \n",
    "\n",
    "    return average_tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is your favourite condiment?\",\n",
    "    \"Do you have any recipes for mayonnaise?\",\n",
    "    \"Once upon a time \",\n",
    "    \"Once upon a time on Mars \",\n",
    "    \"Once upon a time on in the distance past \",\n",
    "]\n",
    "average_tokens_per_second = measure_tokens_per_second(model, tokenizer, prompts, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_tokens_per_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens per second = 44.72630659078358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 2048\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a long story\"},\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "output = model.generate(model_inputs, max_length=model_inputs.shape[1] + max_tokens, do_sample=False)\n",
    "\n",
    "decoded = tokenizer.batch_decode(output)\n",
    "\n",
    "print (decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def download_model_to_folder(model_name: str, folder_path: str):\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    # Download the model and tokenizer to the specified folder\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=folder_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=folder_path)\n",
    "    \n",
    "    print(f\"Model and tokenizer downloaded to {folder_path}\")\n",
    "\n",
    "# Example usage\n",
    "model_name = \"l3utterfly/mistral-7b-v0.1-layla-v4-chatml-gguf\"\n",
    "folder_path = \"./models/layla\"\n",
    "download_model_to_folder(model_name, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./models/gpt2/models--gpt2/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./models/gpt2/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/l3utterfly/mistral-7b-v0.1-layla-v4-chatml-gguf/resolve/main/mistral-7b-v0.1-layla-v4-chatml-Q5_K.gguf?download=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama cpp server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./models/layla/models--l3utterfly--mistral-7b-v0.1-layla-v4-chatml-gguf/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from mistral-7b-v0.1-layla-v4-chatml-Q5_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 261\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32001 '<|im_end|>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    85.94 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4807.06 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    81.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant. Please give a long and detailed answer.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\\n' + system_message + '<|im_end|>\\n'}}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'models', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant. Please give a long and detailed answer.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\n",
      "' + system_message + '<|im_end|>\n",
      "'}}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "my_model_path = \"mistral-7b-v0.1-layla-v4-chatml-Q5_K.gguf\"\n",
    "CONTEXT_SIZE = 512\n",
    "\n",
    "\n",
    "# LOAD THE MODEL\n",
    "zephyr_model = Llama(\n",
    "                    model_path=my_model_path,\n",
    "                    n_ctx=CONTEXT_SIZE,\n",
    "                    n_gpu_layers=33\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_prompt(\n",
    "        user_prompt,\n",
    "        max_tokens = 100,\n",
    "        temperature = 0.3,\n",
    "        top_p = 0.1,\n",
    "        echo = True,\n",
    "        stop = None):\n",
    "    # Define the parameters\n",
    "    model_output = zephyr_model(\n",
    "        user_prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        echo=echo,\n",
    "        stop=stop,\n",
    "    )\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     112.72 ms\n",
      "llama_print_timings:      sample time =      25.73 ms /   100 runs   (    0.26 ms per token,  3886.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.66 ms /     6 tokens (   18.78 ms per token,    53.26 tokens per second)\n",
      "llama_print_timings:        eval time =     840.86 ms /    99 runs   (    8.49 ms per token,   117.74 tokens per second)\n",
      "llama_print_timings:       total time =    1000.44 ms /   105 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-6edb3409-1e5c-47e9-998a-f436d6e847c8', 'object': 'text_completion', 'created': 1718827589, 'model': 'mistral-7b-v0.1-layla-v4-chatml-Q5_K.gguf', 'choices': [{'text': 'Tell me a long story,\\n\\nTell me a short one.\\n\\nI’ll listen to you,\\n\\nAnd I won’t say no.\\n\\nI’ll listen to your stories,\\n\\nOf love and of hate.\\n\\nI’ll listen to your stories,\\n\\nOf joy and of fate.\\n\\nI’ll listen to your stories,\\n\\nOf life and of death.\\n\\nI’ll listen to your stories,\\n\\nOf hope and of breath.', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 6, 'completion_tokens': 100, 'total_tokens': 106}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_prompt = \"Tell me a long story\"\n",
    "\n",
    "\n",
    "response = generate_text_from_prompt(my_prompt)\n",
    "\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'object', 'created', 'model', 'choices', 'usage'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_structure(d, indent=0, parent_is_list=False):\n",
    "    \"\"\"\n",
    "    Recursively prints the structure of a dictionary, including the type of each object.\n",
    "    \n",
    "    Args:\n",
    "    d (dict): The dictionary to print.\n",
    "    indent (int): The current indentation level (used for recursion).\n",
    "    parent_is_list (bool): Indicates if the parent element is a list.\n",
    "    \"\"\"\n",
    "    prefix = '|' if indent > 0 else ''\n",
    "    for i, (key, value) in enumerate(d.items()):\n",
    "        is_last = (i == len(d) - 1)\n",
    "        if is_last and not parent_is_list:\n",
    "            branch = '└─'\n",
    "        else:\n",
    "            branch = '├─'\n",
    "        \n",
    "        print(f\"{prefix}{'    ' * (indent - 1)}{branch}{key} ({type(value).__name__})\")\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            print_dict_structure(value, indent + 1)\n",
    "        elif isinstance(value, list):\n",
    "            print(f\"{prefix}{'    ' * (indent)}├─[\")\n",
    "            for j, item in enumerate(value):\n",
    "                item_is_last = (j == len(value) - 1)\n",
    "                if item_is_last:\n",
    "                    sub_branch = '└─'\n",
    "                else:\n",
    "                    sub_branch = '├─'\n",
    "                if isinstance(item, dict):\n",
    "                    print(f\"{prefix}{'    ' * (indent + 1)}{sub_branch}item ({type(item).__name__})\")\n",
    "                    print_dict_structure(item, indent + 2, parent_is_list=True)\n",
    "                else:\n",
    "                    print(f\"{prefix}{'    ' * (indent + 1)}{sub_branch}{item} ({type(item).__name__})\")\n",
    "            print(f\"{prefix}{'    ' * (indent)}└─]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├─id (str)\n",
      "├─object (str)\n",
      "├─created (int)\n",
      "├─model (str)\n",
      "├─choices (list)\n",
      "├─[\n",
      "    └─item (dict)\n",
      "|    ├─text (str)\n",
      "|    ├─index (int)\n",
      "|    ├─logprobs (NoneType)\n",
      "|    ├─finish_reason (str)\n",
      "└─]\n",
      "└─usage (dict)\n",
      "|├─prompt_tokens (int)\n",
      "|├─completion_tokens (int)\n",
      "|└─total_tokens (int)\n"
     ]
    }
   ],
   "source": [
    "print_dict_structure(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response[\"choices\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a long story,\n",
      "\n",
      "Tell me a short one.\n",
      "\n",
      "I’ll listen to you,\n",
      "\n",
      "And I won’t say no.\n",
      "\n",
      "I’ll listen to your stories,\n",
      "\n",
      "Of love and of hate.\n",
      "\n",
      "I’ll listen to your stories,\n",
      "\n",
      "Of joy and of fate.\n",
      "\n",
      "I’ll listen to your stories,\n",
      "\n",
      "Of life and of death.\n",
      "\n",
      "I’ll listen to your stories,\n",
      "\n",
      "Of hope and of breath.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"finish_reason\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"usage\"][\"completion_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark llama cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llama_measure_tokens_per_second(prompts: list, max_tokens: int = 500) -> float:\n",
    "\n",
    "    total_tokens_generated = 0\n",
    "    total_time_taken = 0.0\n",
    "\n",
    "\n",
    "    for prompt in prompts:\n",
    "        # Measure the time taken to generate the output\n",
    "        start_time = time.time()\n",
    "        #tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        output = generate_text_from_prompt(prompt, max_tokens=max_tokens)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate the number of tokens generated\n",
    "        num_tokens_generated = output[\"usage\"][\"completion_tokens\"]\n",
    "        \n",
    "        # Calculate the time taken\n",
    "        time_taken = end_time - start_time\n",
    "        \n",
    "        # Update the total tokens and time\n",
    "        total_tokens_generated += num_tokens_generated\n",
    "        total_time_taken += time_taken\n",
    "    \n",
    "    \n",
    "    # Calculate average tokens per second\n",
    "    average_tokens_per_second = total_tokens_generated / total_time_taken\n",
    "    \n",
    "\n",
    "    return average_tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is your favourite condiment?\",\n",
    "    \"Do you have any recipes for mayonnaise?\",\n",
    "    \"Once upon a time \",\n",
    "    \"Once upon a time on Mars \",\n",
    "    \"Once upon a time on in the distance past \",\n",
    "]\n",
    "average_tokens_per_second = lamma_measure_tokens_per_second(prompts, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_tokens_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"threadripper\"\n",
    "port = \"8400\"\n",
    "endpoint = \"generate_text\"\n",
    "\n",
    "payload = {\n",
    "  \"user_prompt\": \"Write me a long story set in the X-Files universe: \\n\",\n",
    "  \"max_tokens\": 10000,\n",
    "  \"temperature\": 0.3,\n",
    "  \"top_p\": 0.1,\n",
    "  \"echo\": True,\n",
    "  \"overlap\" : 500\n",
    "}\n",
    "response = requests.post(f\"http://{host}:{port}/{endpoint}\", json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1847"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()[\"token_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write me a long story set in the X-Files universe: \n",
      "\n",
      "I'm not sure if I can do this, but I'll try.\n",
      "\n",
      "The year is 2015 and Mulder has been missing for over a year now. Scully has been working with the FBI ever since, trying to find him. She's been through hell and back, but she won't give up until she finds her partner.\n",
      "\n",
      "One day, while she's investigating a case in New York City, she gets a call from an old friend of Mulder's. He tells her that he has information about where Mulder might be. Scully is hesitant at first, but eventually agrees to meet him.\n",
      "\n",
      "When they finally meet up, the man tells Scully that he knows where Mulder is, but he can't tell her anything more until she helps him with a case. Scully reluctantly agrees and they head out to investigate.\n",
      "\n",
      "The case turns out to be much bigger than either of them expected. It involves a group of people who are trying to take over the world by using alien technology. They've been kidnapping people and experimenting on them, turning them into mindless drones.\n",
      "\n",
      "Scully and her friend work together to stop the group before they can carry out their plan. In the end, they manage to save Mulder from being one of the victims. He's been held captive for months, but he's still alive.\n",
      "\n",
      "The two of them are reunited and Scully is overjoyed to have him back. They spend the rest of their lives together, fighting against evil and trying to make the world a better place.\n",
      "\n",
      "## Chapter 1: The Abduction\n",
      "\n",
      "Scully was sitting at her desk in the FBI office when she heard a knock on the door. She looked up to see Mulder standing there with a worried expression on his face.\n",
      "\n",
      "\"What's wrong?\" she asked, concern etched into her features.\n",
      "\n",
      "\"I need your help,\" he said, his voice barely above a whisper. \"There's something going on and I don't know what it is.\"\n",
      "\n",
      "Scully stood up from her desk and followed him out of the office. They walked down the hallway in silence, each lost in their own thoughts. Finally, they reached an unmarked door at the end of the hall. Mulder knocked twice before opening it slowly.\n",
      "\n",
      "Inside was a small room with a table in the center and several chairs around it. Sitting at the table were three men dressed in black suits and sunglasses. They all turned to look at them as they entered.\n",
      "\n",
      "\"What do you want?\" one of them asked, his voice cold and unfriendly.\n",
      "\n",
      "Mulder took a deep breath before speaking. \"We need your help,\" he said again. \"There's something going on and we don't know what it is.\"\n",
      "\n",
      "The man looked at him for a moment before nodding slightly. \"Come in,\" he said, gesturing to the chairs around the table.\n",
      "\n",
      "Scully and Mulder sat down, their eyes never leaving the men across from them. They exchanged glances, both of them wondering what was going on.\n",
      "\n",
      "\"We've been kidnapping people,\" one of the men said finally. \"And experimenting on them.\"\n",
      "\n",
      "Scully gasped in shock, her hand flying to her mouth. Mulder looked at her with concern before turning back to the men.\n",
      "\n",
      "\"Why?\" he asked, his voice low and dangerous.\n",
      "\n",
      "The man shrugged. \"We don't know,\" he said again. \"But we need your help.\"\n",
      "\n",
      "Scully and Mulder exchanged another glance, this time more cautious than before. They knew that they couldn't just walk away from this, not when people were being hurt. But what could they do? How could they possibly help these men who had kidnapped and experimented on innocent people?\n",
      "\n",
      "\"We can't just let you go,\" Mulder said finally, his voice firm but calm. \"But we will try to find out what's going on.\"\n",
      "\n",
      "The man nodded again, seeming relieved that they weren't going to turn them in right away. \"Thank you,\" he said sincerely. \"We know we don't deserve your help, but we need it.\"\n",
      "\n",
      "Scully and Mulder exchanged another glance before standing up from the table. They knew that this was just the beginning of a long and difficult journey, but they were ready for whatever came next.\n",
      "\n",
      "\"Let's go,\" Scully said, taking Mulder's hand in hers as they walked out of the diner together. \"We have a lot to do.\"\n",
      "\n",
      "As they left the diner, Mulder couldn't help but feel a sense of hope and determination welling up inside him. He knew that this was going to be tough, but he also knew that with Scully by his side, anything was possible. And as they walked down the street hand in hand, he couldn't help but smile at the thought of what lay ahead for them both.\n",
      "\n",
      "The End\"I love you,\" Mulder said softly, looking into Scully's eyes as they continued their walk.\n",
      "\n",
      "Scully smiled back at him, her own eyes filled with love and affection. \"And I love you too, Mulder.\"\n",
      "\n",
      "They walked on in silence for a while longer, lost in their own thoughts but still connected by the warmth of each other's touch. And as they turned onto the street where their apartment building stood, Mulder couldn't help but feel like anything was possible.\n",
      "\n",
      "\"Let's go home,\" Scully said softly, leading him up the stairs to their apartment door.\n",
      "\n",
      "Mulder followed her inside, closing and locking the door behind them as they made their way into the living room. He sat down on the couch, looking over at Scully who was standing in front of him with a determined look in her eyes.\n",
      "\n",
      "\"What do you want to do first?\" she asked him, her voice filled with anticipation and excitement.\n",
      "\n",
      "Mulder smiled at her, his own heart racing with desire as he stood up from the couch and walked over to her. He wrapped his arms around her waist, pulling her close against him as he kissed her passionately on the lips. Scully moaned softly into his mouth, her hands running through his hair as she returned his kiss with equal fervor.\n",
      "\n",
      "\"I want you,\" Mulder growled against her lips, breaking the kiss for a moment to look into her eyes. \"I want you right now.\"\n",
      "\n",
      "Scully nodded, her own desire evident in her gaze as she reached down and unbuttoned his pants, freeing his erect cock from its confines. She wrapped her hand around him, stroking him slowly at first before increasing the pace, moaning softly as she felt him grow even harder in her grip.\n",
      "\n",
      "\"Oh god,\" Mulder groaned, arching into her touch. \"Scully...\"\n",
      "\n",
      "She leaned forward and took him into her mouth, sucking on the head of his cock while using her tongue to tease and pleasure him. Mulder moaned loudly, his hands tangling in her hair as he thrust up into her mouth involuntarily. Scully bobbed her head up and down on him, taking more of him into her throat with each movement until she could feel the veins pulsing beneath the skin of his cock.\n",
      "\n",
      "\"Oh fuck,\" Mulder groaned, his hips bucking against her face as he neared climax. \"Scully...\"\n",
      "\n",
      "She pulled away from him just in time to avoid being covered in his cum, instead taking hold of his cock and stroking it quickly until he came all over her hand and stomach. He panted heavily, leaning back against the wall as he watched Scully clean herself off with a tissue from her purse.\n",
      "\n",
      "\"That was... amazing,\" Mulder said breathlessly when she finally looked up at him again. \"Thank you.\"\n",
      "\n",
      "Scully smiled softly, reaching out to brush some stray hairs from his forehead before leaning in and kissing him gently on the lips.\n",
      "\n",
      "\"Anytime,\" she whispered against his mouth before pulling away and straightening her clothes. \"Now let's get back to work.\"\n",
      "\n",
      "Mulder chuckled softly as he watched Scully walk away, already thinking about how they could continue this later that night when they were alone in their hotel room. He couldn't wait to see what else she had planned for them...\n"
     ]
    }
   ],
   "source": [
    "print(response.json()[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
