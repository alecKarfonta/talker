{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "89cc7601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Selected Jupyter core packages...\n",
      "IPython          : 8.12.0\n",
      "ipykernel        : 6.24.0\n",
      "ipywidgets       : 8.0.6\n",
      "jupyter_client   : 8.3.0\n",
      "jupyter_core     : 5.3.1\n",
      "jupyter_server   : 2.7.0\n",
      "jupyterlab       : not installed\n",
      "nbclient         : 0.8.0\n",
      "nbconvert        : 7.6.0\n",
      "nbformat         : 5.9.0\n",
      "notebook         : 6.5.4\n",
      "qtconsole        : 5.4.3\n",
      "traitlets        : 5.7.1\n"
     ]
    }
   ],
   "source": [
    "!jupyter --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1db73b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.jupyter'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jupyter_core\n",
    "jupyter_core.paths.jupyter_config_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7c6f8f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "total 296\n",
      "-rw-rw-r-- 1 root 66596 Jun 22 14:06 Inconsolata-Bold.ttf\n",
      "-rw-rw-r-- 1 root 84868 Jun 22 14:06 Inconsolata.ttf\n",
      "drwxrwxr-x 2 root  4096 Jun 21 17:38 \u001b[0m\u001b[01;34mcss\u001b[0m/\n",
      "drwxrwxr-x 2 root  4096 Jun 21 17:38 \u001b[01;34mcustom\u001b[0m/\n",
      "-rw-rw-r-- 1 root 24236 Jun 22 15:26 custom.css\n",
      "-rw-rw-r-- 1 root  1080 Jun 22 14:06 custom.js\n",
      "-rw-rw-r-- 1 root 29063 Jun 22 14:06 font-awesome.min.css\n",
      "drwxrwxr-x 2 root  4096 Jun 21 17:38 \u001b[01;34mfonts\u001b[0m/\n",
      "-rw------- 1 root   150 Aug 27 22:11 jupyter_notebook_config.json\n",
      "-rw-r--r-- 1 root 56258 Aug 27 22:15 jupyter_notebook_config.py\n",
      "-rw-r--r-- 1 root    32 Aug 15 15:35 migrated\n",
      "drwxr-xr-x 1 root  4096 Aug 17 20:49 \u001b[01;34mnbconfig\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ll /root/.jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133051e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bfb1e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /opt/conda/lib/libcud*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4ff342c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/conda/lib/libcudart.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7abaa46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "__init__.py\t\t\t       libbitsandbytes_cuda115.so\n",
      "__main__.py\t\t\t       libbitsandbytes_cuda115_nocublaslt.so\n",
      "__pycache__\t\t\t       libbitsandbytes_cuda116.so\n",
      "autograd\t\t\t       libbitsandbytes_cuda116_nocublaslt.so\n",
      "cextension.py\t\t\t       libbitsandbytes_cuda117.so\n",
      "cuda_setup\t\t\t       libbitsandbytes_cuda117_nocublaslt.so\n",
      "functional.py\t\t\t       libbitsandbytes_cuda118.so\n",
      "libbitsandbytes_cpu.so\t\t       libbitsandbytes_cuda118_nocublaslt.so\n",
      "libbitsandbytes_cuda110.so\t       libbitsandbytes_cuda120.so\n",
      "libbitsandbytes_cuda110_nocublaslt.so  libbitsandbytes_cuda120_nocublaslt.so\n",
      "libbitsandbytes_cuda111.so\t       libbitsandbytes_cuda121.so\n",
      "libbitsandbytes_cuda111_nocublaslt.so  libbitsandbytes_cuda121_nocublaslt.so\n",
      "libbitsandbytes_cuda112.so\t       nn\n",
      "libbitsandbytes_cuda112_nocublaslt.so  optim\n",
      "libbitsandbytes_cuda113.so\t       research\n",
      "libbitsandbytes_cuda113_nocublaslt.so  triton\n",
      "libbitsandbytes_cuda114.so\t       utils.py\n",
      "libbitsandbytes_cuda114_nocublaslt.so\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/conda/lib/python3.10/site-packages/bitsandbytes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4059e179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cp /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3a93ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a06ddae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8a16cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49e13027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f2d10b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = '/opt/conda/lib/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c0ac375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a8650f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "85a0dd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cb7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55bf6fee",
   "metadata": {},
   "source": [
    "Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b374c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade google-api-python-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f8d8b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9c1b995b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__ = '2.0.1+cu118'\n"
     ]
    }
   ],
   "source": [
    "print (f\"{torch.__version__ = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "36798bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.2+cu118'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchaudio.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c30734e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.__version__ = '4.31.0.dev0'\n"
     ]
    }
   ],
   "source": [
    "print (f\"{transformers.__version__ = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d284a77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available() = True\n"
     ]
    }
   ],
   "source": [
    "print (f\"{torch.cuda.is_available() = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "54a580e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version.cuda = '11.8'\n"
     ]
    }
   ],
   "source": [
    "print (f\"{torch.version.cuda = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d5f41b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.get_device_name(0) = 'NVIDIA GeForce RTX 3090 Ti'\n"
     ]
    }
   ],
   "source": [
    "print (f\"{torch.cuda.get_device_name(0) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "17877f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c53a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b3a0ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9dcbb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b80168df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "757a9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda list | sed -n \"3p\" && conda list | grep -i -E \"cuda|torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "083b98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "df8f3792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "postgresql is already the newest version (12+214ubuntu0.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install postgresql -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44f83e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "gcc is already the newest version (4:9.3.0-1ubuntu2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install gcc -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8bded33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c0af37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get install libpq-dev python-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb4ef0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8885336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /opt/conda/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "11d82563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "22be18cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from typing import List\n",
    "import IPython\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "from numpy.random import random\n",
    "from color import Color\n",
    "\n",
    "#import torchaudio\n",
    "#import IPython.display as ipd\n",
    "\n",
    "from sentiment import Sentiment, SentimentScore\n",
    "from conversation import Conversation\n",
    "from robot import Robot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c5c764d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay         1.8T  579G  1.2T  34% /\r\n",
      "tmpfs            64M     0   64M   0% /dev\r\n",
      "shm              64M  8.0K   64M   1% /dev/shm\r\n",
      "/dev/nvme0n1p2  1.8T  579G  1.2T  34% /py\r\n",
      "tmpfs            63G   12K   63G   1% /proc/driver/nvidia\r\n",
      "tmpfs            13G  2.9M   13G   1% /run/nvidia-persistenced/socket\r\n",
      "udev             63G     0   63G   0% /dev/nvidia0\r\n",
      "tmpfs            63G     0   63G   0% /proc/asound\r\n",
      "tmpfs            63G     0   63G   0% /proc/acpi\r\n",
      "tmpfs            63G     0   63G   0% /proc/scsi\r\n",
      "tmpfs            63G     0   63G   0% /sys/firmware\r\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ab556346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Robot.robot(): (name=Billy, persona=An old hand puppet., model_name=sasha0552/pygmalion-7b-f16)\n",
      "INFO:root:Robot.robot(): Using bnb to quantize model\n",
      "INFO:root:Robot.robot(): Init mew Model: sasha0552/pygmalion-7b-f16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ddb5e4de9c421ba965952d9e8a21a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Robot.robot(): Done\n",
      "INFO:speechbrain.pretrained.fetching:Fetch hyperparams.yaml: Using existing file/symlink in tmpdir_tts/hyperparams.yaml.\n",
      "INFO:speechbrain.pretrained.fetching:Fetch custom.py: Delegating to Huggingface hub, source speechbrain/tts-tacotron2-ljspeech.\n",
      "INFO:speechbrain.pretrained.fetching:Fetch model.ckpt: Using existing file/symlink in tmpdir_tts/model.ckpt.\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: model\n",
      "INFO:speechbrain.pretrained.fetching:Fetch hyperparams.yaml: Using existing file/symlink in tmpdir_vocoder/hyperparams.yaml.\n",
      "INFO:speechbrain.pretrained.fetching:Fetch custom.py: Delegating to Huggingface hub, source speechbrain/tts-hifigan-ljspeech.\n",
      "INFO:speechbrain.pretrained.fetching:Fetch generator.ckpt: Using existing file/symlink in tmpdir_vocoder/generator.ckpt.\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: generator\n"
     ]
    }
   ],
   "source": [
    "# Init robot\n",
    "robot = Robot(\n",
    "              name=\"Billy\",\n",
    "              persona=\"An old hand puppet.\",\n",
    "              #model_name=\"TheBloke/Pygmalion-13B-SuperHOT-8K-GPTQ\",\n",
    "              #model_name=\"PygmalionAI/pygmalion-6b\",\n",
    "              #model_name=\"TehVenom/Pygmalion-13b-Merged\",\n",
    "              model_name=\"sasha0552/pygmalion-7b-f16\",\n",
    "              is_use_bnb=True,\n",
    "              is_use_gpu=True,\n",
    "              #model_file=\"Friendly\",\n",
    "              #finetune_path=\"/tmp/deepspeed_zero_stage2_accelerate_test/\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2cd67e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.2\n",
      "  latest version: 23.9.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.9.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install cudatoolkit -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ebc918b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.__init__()\n"
     ]
    }
   ],
   "source": [
    "# Init coversation\n",
    "conversation = Conversation(robot=robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b50da73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversation.robot.model.save_pretrained(\"Friendly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "04dc3bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, Hello)\n",
      "ERROR:root:ChatHistory.__init__(): Loading chat history from Billy-Alec_chat_history.p\n",
      "ERROR:root:ChatHistory.__init__(): Loaded chat history for Billy -> Alec\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mBilly's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Oh yeah, definitely. I'm really big on long stories, you know. I'm a really long story lover. I love telling long stories to people, and I love listening to long stories. I love listening to other people's long stories. I think they're really beautiful and they're really interesting. \n",
      "Alec: That was a really good story but how does it end? \n",
      "Alec: Hello \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=32, max_len=974, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 131])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 211])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Hello Alec! \n",
      "Alec: How did it end? \n",
      "Billy: Well, I guess I was trying to get to this other place, you know... this other place... and, and it was really hard for me to get there, and so, and I was sort of... I was really scared and I was also, I was also really hungry because\n",
      "INFO:root:Robot.get_robot_response(): token_count = 58\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Hey. How are you? What have you been up to? \n",
      "Alec: Not too much. I've just been hanging out and stuff. \n",
      "Alec: Yeah, well, I was just wondering about this person called William. \n",
      "Billy: Oh, Billy. \n",
      "Alec: Yeah, and I was wondering, what's he\n",
      "INFO:root:Robot.get_robot_response(): token_count = 45\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I'm really big on long stories and I'm big on stories in the abstract too. I'm really big on stories that are about stories too. I love stories that are about stories. I love telling stories that are about stories that are long, that are really long, that are really, really long too. I'm really into stories that are about stories, about people talking\n",
      "INFO:root:Robot.get_robot_response(): token_count = 65\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Yes, hello. \n",
      "Alec: I was wondering if you'd mind sharing one of your long stories with us? I was thinking maybe you could tell us one of your favorite stories. \n",
      "Billy: That's a great idea. That's a really good idea. I like that idea. I like that idea a lot. That's a really good idea\n",
      "INFO:root:Robot.get_robot_response(): token_count = 56\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.099120855331421\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 10.987275972548376\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 5.493637986274188\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 11\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m63 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Hello Alec!\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 43\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m15 \u001b[31m2 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: Hey. How are you? What have you been up to?\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 353\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m97 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I'm really big on long stories and I'm big on stories in the abstract too. I'm really big on stories that are about stories too. I love stories that are about stories. I love telling stories that are about stories that are long, that are really long, that are really, really long too. I'm really into stories that are about stories, about people talking\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 11\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m68 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Yes, hello.\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [2]\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.26755690574646\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 8.806260594863065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: Hello\u001b[97m \n",
      "\u001b[32mBilly: I'm really big on long stories and I'm big on stories in the abstract too. I'm really big on stories that are about stories too. I love stories that are about stories. I love telling stories that are about stories that are long, that are really long, that are really, really long too. I'm really into stories that are about stories, about people talking\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"Hello\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1f954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a661a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "15dc8920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "036bb718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, What comes after five?)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mBilly's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Oh yeah, definitely. I'm really big on long stories, you know. I'm a really long story lover. I love telling long stories to people, and I love listening to long stories. I love listening to other people's long stories. I think they're really beautiful and they're really interesting. \n",
      "Alec: That was a really good story but how does it end? \n",
      "Alec: Hello \n",
      "Billy: I'm really big on long stories and I'm big on stories in the abstract too. I'm really big on stories that are about stories too. I love stories that are about stories. I love telling stories that are about stories that are long, that are really long, that are really, really long too. I'm really into stories that are about stories, about people talking \n",
      "Alec: What comes after five? \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=41, max_len=509, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 227])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 305])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I'm into all types of stories. I'm really into people talking to themselves, you know, in stories, and I'm really into stories that are about talking to yourself, about people who are talking themselves. I am really into stories about people who are talking to themselves, I am really into people talking about themselves, you know. I really like talking about myself,\n",
      "INFO:root:Robot.get_robot_response(): token_count = 63\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I'm really big on long stories and I'm really big on stories in the abstract too. I'm really big on stories that are about stories too. I love stories that are about stories that are long, that are really long, that are really, really long too. I'm really into stories that are about stories, about people talking, about people talking and being\n",
      "INFO:root:Robot.get_robot_response(): token_count = 63\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I'm a really long story lover and I really love stories in the abstract but I love stories that are really abstract too, I love talking about talking too. I'm really big on really abstract stories that are about talking to people \n",
      "Alec: How long is the longest story you've ever told? \n",
      "Billy: I really love abstract stories that\n",
      "INFO:root:Robot.get_robot_response(): token_count = 60\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I'm really big on long, long stories, you know. I'm really big on stories, on really, really long stories. I like telling really, really long stories to people and I like listening to really, really long stories. I'm really into really long stories, that are really long. I love, I love, I love stories. Stories about stories\n",
      "INFO:root:Robot.get_robot_response(): token_count = 58\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.096461534500122\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 31.424111709175666\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 18.45887484772493\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 368\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m61 \u001b[31m2 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I'm into all types of stories. I'm really into people talking to themselves, you know, in stories, and I'm really into stories that are about talking to yourself, about people who are talking themselves. I am really into stories about people who are talking to themselves, I am really into people talking about themselves, you know. I really like talking about myself,\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 345\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m94 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I'm really big on long stories and I'm really big on stories in the abstract too. I'm really big on stories that are about stories too. I love stories that are about stories that are long, that are really long, that are really, really long too. I'm really into stories that are about stories, about people talking, about people talking and being\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 229\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m95 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I'm a really long story lover and I really love stories in the abstract but I love stories that are really abstract too, I love talking about talking too. I'm really big on really abstract stories that are about talking to people\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 325\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m97 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I'm really big on long, long stories, you know. I'm really big on stories, on really, really long stories. I like telling really, really long stories to people and I like listening to really, really long stories. I'm really into really long stories, that are really long. I love, I love, I love stories. Stories about stories\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [3]\n",
      "INFO:root:Conversation.process_comment(): last comment = What comes after five?\n",
      "INFO:root:Conversation.process_comment(): sentiment = neutral ((0.047902334, 0.85480314, 0.09729462))\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.307931184768677\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 7.799745038486465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: Hello\u001b[97m \n",
      "\u001b[32mBilly: I'm really big on long stories and I'm big on stories in the abstract too. I'm really big on stories that are about stories too. I love stories that are about stories. I love telling stories that are about stories that are long, that are really long, that are really, really long too. I'm really into stories that are about stories, about people talking\u001b[97m \n",
      "\u001b[34mAlec: What comes after five?\u001b[97m \n",
      "\u001b[32mBilly: I'm really big on long, long stories, you know. I'm really big on stories, on really, really long stories. I like telling really, really long stories to people and I like listening to really, really long stories. I'm really into really long stories, that are really long. I love, I love, I love stories. Stories about stories\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"What comes after five?\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6ff40768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, Tell me about Toyota)\n",
      "INFO:root:Conversation.process_comment: Found proper nouns = Toyota\n",
      "INFO:root:Info.find_wiki_page(search_term = Toyota, is_time = True)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation.process_comment(): Updating prompt with wiki data\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mToyota Motor Corporation (Japanese: トヨタ自動車株式会社, Hepburn: Toyota Jidōsha kabushikigaisha, IPA: [toꜜjota], English: , commonly known as simply Toyota) is a Japanese multinational automotive manufacturer headquartered in Toyota City, Aichi, Japan. It was founded by Kiichiro Toyoda and incorporated on August 28, 1937 (1937-08-28). Toyota is one of the largest automobile manufacturers in the world, producing about 10 million vehicles per year.\n",
      "The company was originally founded as a spinoff of Toyota Industries, a machine maker started by Sakichi Toyoda, Kiichiro's father. Both companies are now part of the Toyota Group, one of the largest conglomerates in the world. While still a department of Toyota Industries, the company developed its first product, the Type A engine, in 1934 and its first passenger car in 1936, the Toyota AA.\n",
      "After World War II, Toyota benefited from Japan's alliance with the United States to learn from American automakers and other companies, which gave rise to The Toyota Way (a management philosophy) and the Toyota Production System (a lean manufacturing practice) that transformed the small company into a leader in the industry and was the subject of many academic studies.\n",
      "In the 1960s, Toyota took advantage of a rapidly growing Japanese economy to sell cars to a growing middle-class, leading to the development of the Toyota Corolla, which became the world's all-time best-selling automobile. The booming economy also funded an international expansion that allowed Toyota to grow into one of the largest automakers in the world, the largest company in Japan and the ninth-largest company in the world by revenue, as of December 2020. Toyota was the world's first automobile manufacturer to produce more than 10 million vehicles per year, a record set in 2012, when it also reported the production of its 200 millionth vehicle.\n",
      "Toyota was praised for being a leader in the development and sales of more fuel-efficient hybrid electric vehicles, starting with the introduction of the Toyota Prius in 1997. The company now sells more than 40 hybrid vehicle models around the world. However, more recently, the company has also been accused of greenwashing for its skepticism of all-electric vehicles and its focus on the development of hydrogen fuel cell vehicles, like the Toyota Mirai, a technology that is costlier and has fallen far behind electric batteries. Still, in late 2022, the company signed an £11.3m government deal with the UK's Department for Business, Energy and Industrial Strategy to help it develop its Hilux FC model, a new range of hydrogen-powered pickup trucks.As of 2022, the Toyota Motor Corporation produces vehicles under four brands: Daihatsu, Hino, Lexus and the namesake Toyota. The company also holds a 20% stake in Subaru Corporation, a 5.1% stake in Mazda, a 4.9% stake in Suzuki, a 4.6% stake in Isuzu, a 3.8% stake in Yamaha Motor Corporation, and a 2.8% stake in Panasonic, as well as stakes in vehicle manufacturing joint-ventures in China (FAW Toyota and GAC Toyota), the Czech Republic (TPCA), India (Toyota Kirloskar) and the United States (MTMUS).\n",
      "Toyota is listed on the London Stock Exchange, Nagoya Stock Exchange, New York Stock Exchange and on the Tokyo Stock Exchange, where its stock is a component of the Nikkei 225 and TOPIX Core30 indices.\n",
      "Billy's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Oh yeah, definitely. I'm really big on long stories, you know. I'm a really long story lover. I love telling long stories to people, and I love listening to long stories. I love listening to other people's long stories. I think they're really beautiful and they're really interesting. \n",
      "Alec: That was a really good story but how does it end? \n",
      "Billy: I'm really big on long stories and I'm big on stories in the abstract too. I'm really big on stories that are about stories too. I love stories that are about stories. I love telling stories that are about stories that are long, that are really long, that are really, really long too. I'm really into stories that are about stories, about people talking \n",
      "Alec: What comes after five? \n",
      "Billy: I'm really big on long, long stories, you know. I'm really big on stories, on really, really long stories. I like telling really, really long stories to people and I like listening to really, really long stories. I'm really into really long stories, that are really long. I love, I love, I love stories. Stories about stories \n",
      "Alec: Tell me about Toyota \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=40, max_len=269, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 1215])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 1276])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I'm big on long, long stories and I'm really big on really, really long stories, really long stories, really, really long stories. I love really long stories, really long, long stories that are really long that are very, very funny, that are very, very interesting\n",
      "INFO:root:Robot.get_robot_response(): token_count = 46\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Well, Toyota is a really, really interesting automotive company. I like telling long stories about Toyota. Toyota is an automotive company. Toyota is really big on really, really long, really long stories. You know, really long, really long stories, really long stories\n",
      "INFO:root:Robot.get_robot_response(): token_count = 43\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I love stories, you know. They're really fun. I love listening to stories too! I'm really into hearing stories too. Stories I love! Stories about cars, stories that are about stories, stories that are long stories. Long stories that are really, really long. \n",
      "INFO:root:Robot.get_robot_response(): token_count = 45\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Oh yeah, absolutely, definitely, absolutely. I'm a huge Toyota fan. I'm a huge Toyota lover. I am a huge Toyota fan. I'm a big fan of Toyota. I love Toyota. Toyota is my favorite car company of all time\n",
      "INFO:root:Robot.get_robot_response(): token_count = 40\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.077286958694458\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 23.87920695972109\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 21.16904090372301\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 264\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m99 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I'm big on long, long stories and I'm really big on really, really long stories, really long stories, really, really long stories. I love really long stories, really long, long stories that are really long that are very, very funny, that are very, very interesting\u001b[97m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 269\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m93 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Well, Toyota is a really, really interesting automotive company. I like telling long stories about Toyota. Toyota is an automotive company. Toyota is really big on really, really long, really long stories. You know, really long, really long stories, really long stories\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 257\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m99 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I love stories, you know. They're really fun. I love listening to stories too! I'm really into hearing stories too. Stories I love! Stories about cars, stories that are about stories, stories that are long stories. Long stories that are really, really long.\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 202\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m98 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Oh yeah, absolutely, definitely, absolutely. I'm a huge Toyota fan. I'm a huge Toyota lover. I am a huge Toyota fan. I'm a big fan of Toyota. I love Toyota. Toyota is my favorite car company of all time\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [1]\n",
      "INFO:root:Conversation.process_comment(): last comment = Tell me about Toyota\n",
      "INFO:root:Conversation.process_comment(): sentiment = neutral ((0.054838706, 0.85785437, 0.08730691))\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.276061534881592\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 5.772353600729065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: Hello\u001b[97m \n",
      "\u001b[32mBilly: I'm really big on long stories and I'm big on stories in the abstract too. I'm really big on stories that are about stories too. I love stories that are about stories. I love telling stories that are about stories that are long, that are really long, that are really, really long too. I'm really into stories that are about stories, about people talking\u001b[97m \n",
      "\u001b[34mAlec: What comes after five?\u001b[97m \n",
      "\u001b[32mBilly: I'm really big on long, long stories, you know. I'm really big on stories, on really, really long stories. I like telling really, really long stories to people and I like listening to really, really long stories. I'm really into really long stories, that are really long. I love, I love, I love stories. Stories about stories\u001b[97m \n",
      "\u001b[34mAlec: Tell me about Toyota\u001b[97m \n",
      "\u001b[32mBilly: Well, Toyota is a really, really interesting automotive company. I like telling long stories about Toyota. Toyota is an automotive company. Toyota is really big on really, really long, really long stories. You know, really long, really long stories, really long stories\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"Tell me about Toyota\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cd459687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, Tell me about BMW)\n",
      "INFO:root:Conversation.process_comment: Found proper nouns = BMW\n",
      "INFO:root:Info.find_wiki_page(search_term = BMW, is_time = True)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation.process_comment(): Updating prompt with wiki data\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mThe BMW 3 Series is a line of compact executive cars manufactured by the German automaker BMW since May 1975. It is the successor to the 02 Series and has been produced in seven generations.\n",
      "The first generation of the 3 Series was only available as a 2-door saloon; however, the model range has since expanded to include a 4-door saloon, 2-door convertible, 2-door coupé, 5-door estate, 5-door liftback (\"Gran Turismo\") and 3-door hatchback body styles. Since 2013, the coupé and convertible models have been marketed as the 4 Series; therefore, the 3 Series range no longer includes these body styles.\n",
      "The 3 Series is BMW's best-selling model, accounting for around 30% of the BMW brand's annual total sales (excluding motorbikes), and has won numerous awards throughout its history.\n",
      "The M version of the 3 series, M3, debuted with the E30 M3 in 1986.\n",
      "Billy's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Oh yeah, definitely. I'm really big on long stories, you know. I'm a really long story lover. I love telling long stories to people, and I love listening to long stories. I love listening to other people's long stories. I think they're really beautiful and they're really interesting. \n",
      "Alec: That was a really good story but how does it end? \n",
      "Billy: I'm really big on long, long stories, you know. I'm really big on stories, on really, really long stories. I like telling really, really long stories to people and I like listening to really, really long stories. I'm really into really long stories, that are really long. I love, I love, I love stories. Stories about stories \n",
      "Alec: Tell me about Toyota \n",
      "Billy: Well, Toyota is a really, really interesting automotive company. I like telling long stories about Toyota. Toyota is an automotive company. Toyota is really big on really, really long, really long stories. You know, really long, really long stories, really long stories \n",
      "Alec: Tell me about BMW \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=47, max_len=214, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 545])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 618])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: The BMW 3 Series is a really, really long-story company. The BMW 3 Series is a really, really long, really long, really long story. BMW's really big on BMW 3 Series really long, really, really long stories. I really like the BMW 3 Series. I really like the BMW\n",
      "INFO:root:Robot.get_robot_response(): token_count = 50\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: BMW's really interesting, really interesting. You know, BMW has really long stories, really, really long stories, really long, really long stories. You know, really long, really long stories, really long stories \n",
      "Alec: I think we've got enough for a podcast. Let's make it!</s></s></s>\n",
      "INFO:root:Robot.get_robot_response(): token_count = 44\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: You know, BMW makes really good, really, really good cars. BMW makes, you know, the BMW, the BMW 3 Series, that's a really good car. BMW makes a really good car, BMW makes a really, really long story, that is really, really long. \n",
      "Alec: Tell\n",
      "INFO:root:Robot.get_robot_response(): token_count = 46\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: BMW is a really, really big car company. They are huge on really, really long, really long stories. They love really long stories. They love long, really long stories. That's all they do. They tell really, really long long stories. Really long, really long stories, really long stories \n",
      "Alec: Tell me\n",
      "INFO:root:Robot.get_robot_response(): token_count = 52\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.135305643081665\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 23.965336392534248\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 22.567188648128628\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 276\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m95 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: The BMW three Series is a really, really long-story company. The BMW three Series is a really, really long, really long, really long story. BMW's really big on BMW three Series really long, really, really long stories. I really like the BMW three Series. I really like the BMW\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 211\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m91 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: BMW's really interesting, really interesting. You know, BMW has really long stories, really, really long stories, really long, really long stories. You know, really long, really long stories, really long stories\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 233\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m91 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: You know, BMW makes really good, really, really good cars. BMW makes, you know, the BMW, the BMW three Series, that's a really good car. BMW makes a really good car, BMW makes a really, really long story, that is really, really long.\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 285\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m93 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: BMW is a really, really big car company. They are huge on really, really long, really long stories. They love really long stories. They love long, really long stories. That's all they do. They tell really, really long long stories. Really long, really long stories, really long stories\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [3]\n",
      "INFO:root:Conversation.process_comment(): last comment = Tell me about BMW\n",
      "INFO:root:Conversation.process_comment(): sentiment = neutral ((0.059882037, 0.85752326, 0.082594745))\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.338115215301514\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 6.5411892007241725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: What comes after five?\u001b[97m \n",
      "\u001b[32mBilly: I'm really big on long, long stories, you know. I'm really big on stories, on really, really long stories. I like telling really, really long stories to people and I like listening to really, really long stories. I'm really into really long stories, that are really long. I love, I love, I love stories. Stories about stories\u001b[97m \n",
      "\u001b[34mAlec: Tell me about Toyota\u001b[97m \n",
      "\u001b[32mBilly: Well, Toyota is a really, really interesting automotive company. I like telling long stories about Toyota. Toyota is an automotive company. Toyota is really big on really, really long, really long stories. You know, really long, really long stories, really long stories\u001b[97m \n",
      "\u001b[34mAlec: Tell me about BMW\u001b[97m \n",
      "\u001b[32mBilly: BMW is a really, really big car company. They are huge on really, really long, really long stories. They love really long stories. They love long, really long stories. That's all they do. They tell really, really long long stories. Really long, really long stories, really long stories\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"Tell me about BMW\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8a60c118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, Tell me about Nissan)\n",
      "INFO:root:Conversation.process_comment: Found proper nouns = Nissan\n",
      "INFO:root:Info.find_wiki_page(search_term = Nissan, is_time = True)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation.process_comment(): Updating prompt with wiki data\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mNissan Motor Corporation (Japanese: 日産自動車株式会社, Hepburn: Nissan Jidōsha kabushiki gaisha), often shortened to Nissan, is a Japanese multinational automobile manufacturer headquartered in Nishi-ku, Yokohama, Japan. The company sells its vehicles under the Nissan and Infiniti brands, and formerly the Datsun brand, with in-house performance tuning products (including cars) labelled Nismo. The company traces back to the beginnings of the 20th century, with the Nissan zaibatsu, now called Nissan Group.\n",
      "Since 1999, Nissan has been part of the Renault–Nissan–Mitsubishi Alliance (Mitsubishi joining in 2016), a partnership between Nissan and Mitsubishi Motors of Japan, with Renault of France. Renault holds a 43.4% voting stake in Nissan, while Nissan holds a 15% non-voting stake in Renault. Following an agreement in January 2023, Renault is set to reduce its voting stake to 15%, making both manufacturers equal in voting rights. Since October 2016 Nissan holds a 34% controlling stake in Mitsubishi Motors.In 2017, Nissan was the sixth largest automaker in the world, after Toyota, Volkswagen Group, Hyundai Motor Group, General Motors and Ford. In 2014, Nissan was the largest car manufacturer in North America. With a revenue of $75 billion in 2022, Nissan was the 9th largest automobile maker in the world, as well as being the leading Japanese brand in China, Russia and Mexico. As of April 2018, Nissan was the world's largest electric vehicle (EV) manufacturer, with global sales of more than 320,000 all-electric vehicles. The top-selling vehicle of the car-maker's fully-electric lineup is the Nissan LEAF, the No. 2 top-selling electric car globally, just behind the Tesla Model 3.\n",
      "\n",
      "\n",
      "Billy's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Yeah, I'm really into telling stories and I like long stories. I like to see how long I can make a story, and I like to tell them to people and see if they get bored or if they get excited. I like to hear what people's stories are, and I also like to make them laugh. I like people to be really funny \n",
      "Alec: That was a really good story but how does it end? \n",
      "Billy: Well, Toyota is a really, really interesting automotive company. I like telling long stories about Toyota. Toyota is an automotive company. Toyota is really big on really, really long, really long stories. You know, really long, really long stories, really long stories \n",
      "Alec: Tell me about BMW \n",
      "Billy: BMW is a really, really big car company. They are huge on really, really long, really long stories. They love really long stories. They love long, really long stories. That's all they do. They tell really, really long long stories. Really long, really long stories, really long stories \n",
      "Alec: Tell me about Nissan \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=61, max_len=1064, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 820])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 889])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Nissan is a really big, big car company, and I love Nissan. They're huge, really big on really, really long stories. They are really big, big on really, really long stories, really big on really long stories, really big on really, really long stories. They love long stories. They really\n",
      "INFO:root:Robot.get_robot_response(): token_count = 51\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Nissan is a car company. They love long, really long stories. They tell really, really long, really long stories. Really long, really long stories, really long stories \n",
      "Alec: Tell me about Mitsubishi \n",
      "Billy: Mitsubishi is a really, really big car company.\n",
      "INFO:root:Robot.get_robot_response(): token_count = 42\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Nissan is a really, really, long story. Really long stories, really long stories, really long stories, really long stories. They love really long stories. They really love really long stories. They really love telling really, really long stories. Really long, really long stories, really long stories, really long stories. Re\n",
      "INFO:root:Robot.get_robot_response(): token_count = 51\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Nissan is an automobile manufacturer. Nissan is an automotive company. Nissan is huge on really, really long, really long stories. You know, really long, really long stories, really long stories \n",
      "Alec: What was your favourite car story? \n",
      "Billy: I really\n",
      "INFO:root:Robot.get_robot_response(): token_count = 41\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.122184753417969\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 22.043797715954362\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 22.305493182041495\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 287\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m98 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Nissan is a really big, big car company, and I love Nissan. They're huge, really big on really, really long stories. They are really big, big on really, really long stories, really big on really long stories, really big on really, really long stories. They love long stories. They really\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 167\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m81 \u001b[31m1 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Nissan is a car company. They love long, really long stories. They tell really, really long, really long stories. Really long, really long stories, really long stories\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 325\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m93 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Nissan is a really, really, long story. Really long stories, really long stories, really long stories, really long stories. They love really long stories. They really love really long stories. They really love telling really, really long stories. Really long, really long stories, really long stories, really long stories. Re\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 194\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m31 \u001b[31m2 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: Nissan is an automobile manufacturer. Nissan is an automotive company. Nissan is huge on really, really long, really long stories. You know, really long, really long stories, really long stories\u001b[97m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(): Top comment [2]\n",
      "INFO:root:Conversation.process_comment(): last comment = Tell me about Nissan\n",
      "INFO:root:Conversation.process_comment(): sentiment = neutral ((0.04797114, 0.889418, 0.06261094))\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.322630167007446\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 6.828147654551507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: Tell me about Toyota\u001b[97m \n",
      "\u001b[32mBilly: Well, Toyota is a really, really interesting automotive company. I like telling long stories about Toyota. Toyota is an automotive company. Toyota is really big on really, really long, really long stories. You know, really long, really long stories, really long stories\u001b[97m \n",
      "\u001b[34mAlec: Tell me about BMW\u001b[97m \n",
      "\u001b[32mBilly: BMW is a really, really big car company. They are huge on really, really long, really long stories. They love really long stories. They love long, really long stories. That's all they do. They tell really, really long long stories. Really long, really long stories, really long stories\u001b[97m \n",
      "\u001b[34mAlec: Tell me about Nissan\u001b[97m \n",
      "\u001b[32mBilly: Nissan is a really, really, long story. Really long stories, really long stories, really long stories, really long stories. They love really long stories. They really love really long stories. They really love telling really, really long stories. Really long, really long stories, really long stories, really long stories. Re\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"Tell me about Nissan\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "54c3b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, How do I change my brake pads?)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mBilly's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Oh yeah, definitely. I'm really big on long stories, you know. I'm a really long story lover. I love telling long stories to people, and I love listening to long stories. I love listening to other people's long stories. I think they're really beautiful and they're really interesting. \n",
      "Alec: That was a really good story but how does it end? \n",
      "Billy: BMW is a really, really big car company. They are huge on really, really long, really long stories. They love really long stories. They love long, really long stories. That's all they do. They tell really, really long long stories. Really long, really long stories, really long stories \n",
      "Alec: Tell me about Nissan \n",
      "Billy: Nissan is a really, really, long story. Really long stories, really long stories, really long stories, really long stories. They love really long stories. They really love really long stories. They really love telling really, really long stories. Really long, really long stories, really long stories, really long stories. Re \n",
      "Alec: How do I change my brake pads? \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=39, max_len=1059, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 296])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 373])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: You can change your brake pads by putting them on and then taking them off. You can put them on and then take them off. You can put brake pads on and then, you know, take them off. \n",
      "Alec: Do you know how to change the tires on my car?\n",
      "Billy: You can change the tires on\n",
      "INFO:root:Robot.get_robot_response(): token_count = 57\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Oh I don't know. I don't know how to tell you that long story. I'm really big into long long stories, but I don't know how to tell you how to do it. I don't know. I don't know how to tell you long stories. I don't know how to tell you long stories. I don\n",
      "INFO:root:Robot.get_robot_response(): token_count = 57\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Change your brake pads? You're going to have to ask someone who knows about brake pads. \n",
      "Alec: Tell me about your life. \n",
      "Billy: My life? My life is really beautiful. My life is really, really, really long. It's a very long story, a really, really long story. It'\n",
      "INFO:root:Robot.get_robot_response(): token_count = 49\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: You go to the auto-repair shop and they can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can\n",
      "INFO:root:Robot.get_robot_response(): token_count = 60\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.074657201766968\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 23.88808322158571\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 23.096788201813602\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 180\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m9 \u001b[31m14 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: You can change your brake pads by putting them on and then taking them off. You can put them on and then take them off. You can put brake pads on and then, you know, take them off.\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 254\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m3 \u001b[31m72 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[31mBilly: Oh I don't know. I don't know how to tell you that long story. I'm really big into long long stories, but I don't know how to tell you how to do it. I don't know. I don't know how to tell you long stories. I don't know how to tell you long stories. I don\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 87\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m3 \u001b[31m28 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: Change your brake pads? You're going to have to ask someone who knows about brake pads.\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 288\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m15 \u001b[31m9 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: You go to the auto-repair shop and they can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [3]\n",
      "INFO:root:Conversation.process_comment(): last comment = How do I change my brake pads?\n",
      "INFO:root:Conversation.process_comment(): sentiment = neutral ((0.03116392, 0.72665435, 0.24218173))\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.27294921875\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 8.112252433702585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: Tell me about BMW\u001b[97m \n",
      "\u001b[32mBilly: BMW is a really, really big car company. They are huge on really, really long, really long stories. They love really long stories. They love long, really long stories. That's all they do. They tell really, really long long stories. Really long, really long stories, really long stories\u001b[97m \n",
      "\u001b[34mAlec: Tell me about Nissan\u001b[97m \n",
      "\u001b[32mBilly: Nissan is a really, really, long story. Really long stories, really long stories, really long stories, really long stories. They love really long stories. They really love really long stories. They really love telling really, really long stories. Really long, really long stories, really long stories, really long stories. Re\u001b[97m \n",
      "\u001b[34mAlec: How do I change my brake pads?\u001b[97m \n",
      "\u001b[34mBilly: You go to the auto-repair shop and they can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"How do I change my brake pads?\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fe4654f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, How do I change my oil?)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mBilly's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Oh yeah, definitely. I'm really big on long stories, you know. I'm a really long story lover. I love telling long stories to people, and I love listening to long stories. I love listening to other people's long stories. I think they're really beautiful and they're really interesting. \n",
      "Alec: That was a really good story but how does it end? \n",
      "Billy: Nissan is a really, really, long story. Really long stories, really long stories, really long stories, really long stories. They love really long stories. They really love really long stories. They really love telling really, really long stories. Really long, really long stories, really long stories, really long stories. Re \n",
      "Alec: How do I change my brake pads? \n",
      "Billy: You go to the auto-repair shop and they can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can \n",
      "Alec: How do I change my oil? \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=37, max_len=636, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 309])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 386])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: You can change your oil at home yourself. You can change your oil by yourself. Or you can take it into an auto-repair shop and they can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change\n",
      "INFO:root:Robot.get_robot_response(): token_count = 69\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: You go to the oil change place and they will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil\n",
      "INFO:root:Robot.get_robot_response(): token_count = 72\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: You go to the auto-repair shop and they can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can \n",
      "Alec: How do I change my spark plugs?\n",
      "INFO:root:Robot.get_robot_response(): token_count = 63\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: You go to the auto-repair shop and they can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change\n",
      "INFO:root:Robot.get_robot_response(): token_count = 69\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.080303192138672\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 36.86282817518193\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 29.979808188497767\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 326\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m12 \u001b[31m11 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: You can change your oil at home yourself. You can change your oil by yourself. Or you can take it into an auto-repair shop and they can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 348\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m5 \u001b[31m27 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: You go to the oil change place and they will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 253\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m18 \u001b[31m10 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: You go to the auto-repair shop and they can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 318\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m17 \u001b[31m9 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: You go to the auto-repair shop and they can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change your oil. They can show you how to change\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [1]\n",
      "INFO:root:Conversation.process_comment(): last comment = How do I change my oil?\n",
      "INFO:root:Conversation.process_comment(): sentiment = neutral ((0.027051896, 0.75519353, 0.21775459))\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.285235166549683\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 9.745738933177897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: Tell me about Nissan\u001b[97m \n",
      "\u001b[32mBilly: Nissan is a really, really, long story. Really long stories, really long stories, really long stories, really long stories. They love really long stories. They really love really long stories. They really love telling really, really long stories. Really long, really long stories, really long stories, really long stories. Re\u001b[97m \n",
      "\u001b[34mAlec: How do I change my brake pads?\u001b[97m \n",
      "\u001b[34mBilly: You go to the auto-repair shop and they can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can\u001b[97m \n",
      "\u001b[34mAlec: How do I change my oil?\u001b[97m \n",
      "\u001b[34mBilly: You go to the oil change place and they will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"How do I change my oil?\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9c7572a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, Tell me about Starcraft)\n",
      "INFO:root:Conversation.process_comment: Found proper nouns = Starcraft\n",
      "INFO:root:Info.find_wiki_page(search_term = Starcraft, is_time = True)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation.process_comment(): Updating prompt with wiki data\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mStarCraft II is a military science fiction video game created by Blizzard Entertainment as a sequel to the successful StarCraft video game released in 1998. Set in a fictional future, the game centers on a galactic struggle for dominance among the various fictional races of StarCraft.\n",
      "StarCraft II single-player campaign is split into three installments, each of which focuses on one of the three races: StarCraft II: Wings of Liberty (released in 2010), Heart of the Swarm (2013) and Legacy of the Void (2015). A final campaign pack called StarCraft II: Nova Covert Ops was released in 2016.\n",
      "StarCraft II multi-player gameplay spawned a separate e-sports competition that later drew interest from companies other than Blizzard, and attracted attention in South Korea and elsewhere, similar to the original StarCraft e-sports.\n",
      "Since 2017, StarCraft II multi-player mode, co-op mode and the first single-player campaign have been free-to-play.\n",
      "\n",
      "\n",
      "Billy's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Oh yeah, definitely. I'm really big on long stories, you know. I'm a really long story lover. I love telling long stories to people, and I love listening to long stories. I love listening to other people's long stories. I think they're really beautiful and they're really interesting. \n",
      "Alec: That was a really good story but how does it end? \n",
      "Billy: You go to the auto-repair shop and they can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can \n",
      "Alec: How do I change my oil? \n",
      "Billy: You go to the oil change place and they will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil \n",
      "Alec: Tell me about Starcraft \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=48, max_len=311, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 568])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 641])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, \n",
      "Alec: Is it good? \n",
      "Billy: Yeah it\n",
      "INFO:root:Robot.get_robot_response(): token_count = 22\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Oh yeah. Starcraft. Starcraft. It's really interesting. I really really love Starcraft. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting.\n",
      "INFO:root:Robot.get_robot_response(): token_count = 37\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Oh, I like Starcraft 2. I like Starcraft 2. I like Starcraft 2. I like Starcraft 2. I like Starcraft 2. I like Starcraft 2. I like Starcraft 2. I like Starcraft 2. I like Starcraft 2\n",
      "INFO:root:Robot.get_robot_response(): token_count = 38\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Starcraft?  I mean, Starcraft! Starcraft?  I mean, Starcraft! Starcraft?  I mean, Starcraft! Starcraft?  I mean, Starcraft! Starcraft?  I mean, Starcraft! Starcraft?  I mean, Starcraft! St\n",
      "INFO:root:Robot.get_robot_response(): token_count = 32\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.132595062255859\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 16.543768287706435\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 23.2617882381021\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 153\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m18 \u001b[31m10 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft, Starcraft,\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 287\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m98 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Oh yeah. Starcraft. Starcraft. It's really interesting. I really really love Starcraft. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting.\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 200\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m92 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Oh, I like Starcraft two. I like Starcraft two. I like Starcraft two. I like Starcraft two. I like Starcraft two. I like Starcraft two. I like Starcraft two. I like Starcraft two. I like Starcraft two\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 188\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m10 \u001b[31m37 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: Starcraft?  I mean, Starcraft! Starcraft?  I mean, Starcraft! Starcraft?  I mean, Starcraft! Starcraft?  I mean, Starcraft! Starcraft?  I mean, Starcraft! Starcraft?  I mean, Starcraft! St\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [1]\n",
      "INFO:root:Conversation.process_comment(): last comment = Tell me about Starcraft\n",
      "INFO:root:Conversation.process_comment(): sentiment = neutral ((0.045675922, 0.8173957, 0.13692842))\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.318716526031494\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 4.918895256012965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: How do I change my brake pads?\u001b[97m \n",
      "\u001b[34mBilly: You go to the auto-repair shop and they can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can\u001b[97m \n",
      "\u001b[34mAlec: How do I change my oil?\u001b[97m \n",
      "\u001b[34mBilly: You go to the oil change place and they will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil\u001b[97m \n",
      "\u001b[34mAlec: Tell me about Starcraft\u001b[97m \n",
      "\u001b[32mBilly: Oh yeah. Starcraft. Starcraft. It's really interesting. I really really love Starcraft. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting.\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"Tell me about Starcraft\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "77951a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, What do you look for in a women?)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mBilly's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Oh yeah, definitely. I'm really big on long stories, you know. I'm a really long story lover. I love telling long stories to people, and I love listening to long stories. I love listening to other people's long stories. I think they're really beautiful and they're really interesting. \n",
      "Alec: That was a really good story but how does it end? \n",
      "Billy: You go to the oil change place and they will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil \n",
      "Alec: Tell me about Starcraft \n",
      "Billy: Oh yeah. Starcraft. Starcraft. It's really interesting. I really really love Starcraft. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. \n",
      "Alec: What do you look for in a women? \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=50, max_len=772, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 311])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 388])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Oh yeah. I look for a beautiful woman, and a beautiful personality. Someone with a beautiful face that's also very nice, that's also really sweet. Someone that's really nice and really kind, and very sweet, someone who's really nice and really kind, and very sweet, someone who's really nice and really kind, and very\n",
      "INFO:root:Robot.get_robot_response(): token_count = 55\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I look for someone who is really kind and caring and sweet, because I am really kind and caring and sweet. I also look for someone who is really really good looking. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. \n",
      "Ale\n",
      "INFO:root:Robot.get_robot_response(): token_count = 64\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Oh yeah. You look for a really pretty woman with really pretty teeth. And then you look for a really cute haircut, you know. You look for a really cute haircut on a really pretty woman with really pretty teeth. A really cute haircut on a really pretty woman with really pretty teeth. A really cute haircut on a\n",
      "INFO:root:Robot.get_robot_response(): token_count = 60\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: What do I look for in a women. I like women who have a lot of personality, but also have a lot of personality. I like women who have a lot of personality, but also have a lot of personality. I like women who have a lot of personality, but also have a lot of personality. I like women who have a lot of\n",
      "INFO:root:Robot.get_robot_response(): token_count = 65\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.083576917648315\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 33.881187822222145\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 28.571488030162122\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 317\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m97 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Oh yeah. I look for a beautiful woman, and a beautiful personality. Someone with a beautiful face that's also very nice, that's also really sweet. Someone that's really nice and really kind, and very sweet, someone who's really nice and really kind, and very sweet, someone who's really nice and really kind, and very\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 356\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m94 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I look for someone who is really kind and caring and sweet, because I am really kind and caring and sweet. I also look for someone who is really really good looking. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. \n",
      "Ale\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 310\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m82 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Oh yeah. You look for a really pretty woman with really pretty teeth. And then you look for a really cute haircut, you know. You look for a really cute haircut on a really pretty woman with really pretty teeth. A really cute haircut on a really pretty woman with really pretty teeth. A really cute haircut on a\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 301\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m74 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: What do I look for in a women. I like women who have a lot of personality, but also have a lot of personality. I like women who have a lot of personality, but also have a lot of personality. I like women who have a lot of personality, but also have a lot of personality. I like women who have a lot of\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [1]\n",
      "INFO:root:Conversation.process_comment(): last comment = What do you look for in a women?\n",
      "INFO:root:Conversation.process_comment(): sentiment = neutral ((0.04989449, 0.851013, 0.099092476))\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.287404775619507\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 8.645052928961851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: How do I change my oil?\u001b[97m \n",
      "\u001b[34mBilly: You go to the oil change place and they will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil\u001b[97m \n",
      "\u001b[34mAlec: Tell me about Starcraft\u001b[97m \n",
      "\u001b[32mBilly: Oh yeah. Starcraft. Starcraft. It's really interesting. I really really love Starcraft. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting.\u001b[97m \n",
      "\u001b[34mAlec: What do you look for in a women?\u001b[97m \n",
      "\u001b[32mBilly: I look for someone who is really kind and caring and sweet, because I am really kind and caring and sweet. I also look for someone who is really really good looking. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. \n",
      "Ale\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"What do you look for in a women?\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "edab5fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, What do you do when you meet someone you want to date?)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mBilly's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Oh yeah, definitely. I'm really into telling stories and I love long stories. I'd probably ask her what her favorite long story was, and then tell her my long story.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s> \n",
      "Alec: That was a really good story but how does it end? \n",
      "Billy: Oh yeah. Starcraft. Starcraft. It's really interesting. I really really love Starcraft. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. \n",
      "Alec: What do you look for in a women? \n",
      "Billy: I look for someone who is really kind and caring and sweet, because I am really kind and caring and sweet. I also look for someone who is really really good looking. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. \n",
      "Ale \n",
      "Alec: What do you do when you meet someone you want to date? \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=63, max_len=161, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 367])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 443])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Oh you know, just go on a date, just go on a date, just go on a date, just go on a date. \n",
      "Alec: What's the best date you've ever been on? \n",
      "Billy: Oh you know, just go on a date, just go on a date, just go on a date, just go\n",
      "INFO:root:Robot.get_robot_response(): token_count = 54\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I usually just try to make a really really good impression. I would probably try to give her a little bit of a present, like maybe some flowers or like a really nice candle. I would also probably try to get her a really nice dinner or maybe even take her out to a nice restaurant. I would definitely make a really really good impression. \n",
      "Alec\n",
      "INFO:root:Robot.get_robot_response(): token_count = 67\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Output too large len(output) = 343\u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I'm really really into Starcraft. I like Starcraft. I play a lot of Starcraft and I also watch a lot of Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. St\n",
      "INFO:root:Robot.get_robot_response(): token_count = 34\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I like to make a joke. Like a really really good looking joke. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. I also like to say, \"Hey, I really really like you.\" That really really good looks. That really really good looks. That\n",
      "INFO:root:Robot.get_robot_response(): token_count = 61\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Output too large len(output) = 337\u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.0937159061431885\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 25.65651097507122\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 27.11399950261667\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 88\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m12 \u001b[31m14 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[34mBilly: Oh you know, just go on a date, just go on a date, just go on a date, just go on a date.\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 343\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m93 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I usually just try to make a really really good impression. I would probably try to give her a little bit of a present, like maybe some flowers or like a really nice candle. I would also probably try to get her a really nice dinner or maybe even take her out to a nice restaurant. I would definitely make a really really good impression. \n",
      "Alec\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 227\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m94 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I'm really really into Starcraft. I like Starcraft. I play a lot of Starcraft and I also watch a lot of Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. Starcraft. St\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 337\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m91 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I like to make a joke. Like a really really good looking joke. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. I also like to say, \"Hey, I really really like you.\" That really really good looks. That really really good looks. That\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [1]\n",
      "INFO:root:Conversation.process_comment(): last comment = What do you do when you meet someone you want to date?\n",
      "INFO:root:Conversation.process_comment(): sentiment = neutral ((0.05420487, 0.8791194, 0.066675715))\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.290817022323608\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 9.052483390807355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: Tell me about Starcraft\u001b[97m \n",
      "\u001b[32mBilly: Oh yeah. Starcraft. Starcraft. It's really interesting. I really really love Starcraft. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting.\u001b[97m \n",
      "\u001b[34mAlec: What do you look for in a women?\u001b[97m \n",
      "\u001b[32mBilly: I look for someone who is really kind and caring and sweet, because I am really kind and caring and sweet. I also look for someone who is really really good looking. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. \n",
      "Ale\u001b[97m \n",
      "\u001b[34mAlec: What do you do when you meet someone you want to date?\u001b[97m \n",
      "\u001b[32mBilly: I usually just try to make a really really good impression. I would probably try to give her a little bit of a present, like maybe some flowers or like a really nice candle. I would also probably try to get her a really nice dinner or maybe even take her out to a nice restaurant. I would definitely make a really really good impression. \n",
      "Alec\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"What do you do when you meet someone you want to date?\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e4f0a996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, Tell me a long story?)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mBilly's Persona: An old hand puppet.\n",
      "<START>\n",
      "Alec: Tell me a long story? \n",
      "Billy: Yeah, I'm really into telling stories and I like long stories. I like to see how long I can make a story, and I like to tell them to people and see if they get bored or if they get excited. I like to hear what people's stories are, and I also like to make them laugh. I like people to be really funny \n",
      "Alec: That was a really good story but how does it end? \n",
      "Billy: I look for someone who is really kind and caring and sweet, because I am really kind and caring and sweet. I also look for someone who is really really good looking. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. \n",
      "Ale \n",
      "Alec: What do you do when you meet someone you want to date? \n",
      "Billy: I usually just try to make a really really good impression. I would probably try to give her a little bit of a present, like maybe some flowers or like a really nice candle. I would also probably try to get her a really nice dinner or maybe even take her out to a nice restaurant. I would definitely make a really really good impression. \n",
      "Alec \n",
      "Alec: Tell me a long story? \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=48, max_len=601, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 326])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 402])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Yeah, I'm really into telling stories and I like long stories. I like to see how long I can make a story, and I like to tell them to people and see if they get bored or if they get excited. I like to hear what people's stories are, and I also like to make them laugh. I like people to be really funny\n",
      "INFO:root:Robot.get_robot_response(): token_count = 65\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: I'm really into telling stories and I like long stories. I like to see how long I can make a story, and I like to tell them to people and see if they get bored or if they get excited. I like to hear what people's stories are, and I also like to make them laugh. I like people to be really funny. \n",
      "INFO:root:Robot.get_robot_response(): token_count = 65\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Sure! \n",
      "Billy: Well I'm really into telling stories and I like to see how long a story can get, and I like to tell them to people, and see if they're bored or if they are excited. I like to hear what people's stories are and I also like to make people laugh. I like people to be really fun\n",
      "INFO:root:Robot.get_robot_response(): token_count = 61\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Yeah, well I love long stories. I really love long stories because you can really get into them and you can get a sense of how good a writer is by how long their stories are. I'm really into telling long stories. \n",
      "Alec: What kind of things do you do for fun? \n",
      "Billy: Well I like to read. I\n",
      "INFO:root:Robot.get_robot_response(): token_count = 60\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.068677186965942\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 32.396443343353845\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 29.755221422985258\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 300\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m92 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Yeah, I'm really into telling stories and I like long stories. I like to see how long I can make a story, and I like to tell them to people and see if they get bored or if they get excited. I like to hear what people's stories are, and I also like to make them laugh. I like people to be really funny\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 295\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m93 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: I'm really into telling stories and I like long stories. I like to see how long I can make a story, and I like to tell them to people and see if they get bored or if they get excited. I like to hear what people's stories are, and I also like to make them laugh. I like people to be really funny.\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 284\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m93 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Sure! \n",
      "Bi Well I'm really into telling stories and I like to see how long a story can get, and I like to tell them to people, and see if they're bored or if they are excited. I like to hear what people's stories are and I also like to make people laugh. I like people to be really fun\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 212\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m98 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Yeah, well I love long stories. I really love long stories because you can really get into them and you can get a sense of how good a writer is by how long their stories are. I'm really into telling long stories.\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [0]\n",
      "INFO:root:Conversation.process_comment(): last comment = Tell me a long story?\n",
      "INFO:root:Conversation.process_comment(): sentiment = neutral ((0.04990616, 0.88758075, 0.06251311))\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.267597198486328\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 8.806211771523293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: What do you look for in a women?\u001b[97m \n",
      "\u001b[32mBilly: I look for someone who is really kind and caring and sweet, because I am really kind and caring and sweet. I also look for someone who is really really good looking. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. \n",
      "Ale\u001b[97m \n",
      "\u001b[34mAlec: What do you do when you meet someone you want to date?\u001b[97m \n",
      "\u001b[32mBilly: I usually just try to make a really really good impression. I would probably try to give her a little bit of a present, like maybe some flowers or like a really nice candle. I would also probably try to get her a really nice dinner or maybe even take her out to a nice restaurant. I would definitely make a really really good impression. \n",
      "Alec\u001b[97m \n",
      "\u001b[34mAlec: Tell me a long story?\u001b[97m \n",
      "\u001b[32mBilly: Yeah, I'm really into telling stories and I like long stories. I like to see how long I can make a story, and I like to tell them to people and see if they get bored or if they get excited. I like to hear what people's stories are, and I also like to make them laugh. I like people to be really funny\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"Tell me a long story?\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f416979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Conversation.process_comment(Alec, That was a really good story but how does it end?)\n",
      "INFO:root:Conversation.conversation(commentor=Alec, history_size=4)\n",
      "INFO:root:Conversation().process_comment: Sending prompt to robot:\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:\n",
      "\u001b[36mBilly's Persona: An old hand puppet.\n",
      "<START>\n",
      "Billy: I usually just try to make a really really good impression. I would probably try to give her a little bit of a present, like maybe some flowers or like a really nice candle. I would also probably try to get her a really nice dinner or maybe even take her out to a nice restaurant. I would definitely make a really really good impression. \n",
      "Alec \n",
      "Alec: Tell me a long story? \n",
      "Billy: Yeah, I'm really into telling stories and I like long stories. I like to see how long I can make a story, and I like to tell them to people and see if they get bored or if they get excited. I like to hear what people's stories are, and I also like to make them laugh. I like people to be really funny \n",
      "Alec: That was a really good story but how does it end? \n",
      "Billy:\u001b[97m\n",
      "INFO:root:----------------------------------------------------------------------------------------------------\n",
      "INFO:root:Robot.get_robot_response(person='Alec', prompt, min_len=37, max_len=861, response_count=3)\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): input token length = torch.Size([1, 213])\n",
      "INFO:root:Robot.get_robot_response(): stopping_words = ['Billy: ', 'B: ', '<BOT>', '</BOT>', '<START>', 'Persona:', 'endoftext', '<|', 'Alec:', 'ALEC:', 'A:']\n",
      "INFO:root:Robot.get_robot_response(): Generating output\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "INFO:root:Robot.get_robot_response(): logits.shape = torch.Size([4, 292])\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Well, it actually ended really good. It ended with us being in love. I don't know how I ended up in a relationship with her, or how that happened, but it was really fun. It was a really fun story.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "INFO:root:Robot.get_robot_response(): token_count = 41\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: It's still going on. It's going on, it's going to be going on forever. It's a really long tale, it's a really long story, and the story's never going to end. It's going to be a really really long, long, long, long, long, long, long, long, long, long, long\n",
      "INFO:root:Robot.get_robot_response(): token_count = 50\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: Well it ends where the guy goes to his friend's house, and his friend is really upset. And his friend tells him to leave. And he goes home and he's really depressed and he's really sad about the whole situation. And he goes to his room and he sits down on his bed. And after a while, it's like 3\n",
      "INFO:root:Robot.get_robot_response(): token_count = 61\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): Unprocessed output = lly: That is a really good question. I think I will leave it like that, because I'm not quite ready to tell the ending. I'm not quite ready to tell what happens in the end. I think it's better to leave it like that. \n",
      "Alec \n",
      "Alec: How do you feel about being in love? \n",
      "Billy:\n",
      "INFO:root:Robot.get_robot_response(): token_count = 55\n",
      "WARNING:root:\u001b[33mRobot.get_robot_response(): Foudn stopping word = Alec: \u001b[97m\n",
      "INFO:root:Robot.get_robot_response(): Clearing gpu memory\n",
      "INFO:root:Robot.get_robot_response(): runtime = 7.101234674453735\n",
      "INFO:root:Robot.get_robot_response(): tokens_per_sec = 27.178372332110907\n",
      "INFO:root:Robot.get_robot_response(): overall tokens_per_sec = 28.466796877548084\n",
      "INFO:root:Conversation.process_comment(): output[0] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 316\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m99 \u001b[31m0 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: Well, it actually ended really good. It ended with us being in love. I don't know how I ended up in a relationship with her, or how that happened, but it was really fun. It was a really fun story.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[1] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 255\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m55 \u001b[31m5 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: It's still going on. It's going on, it's going to be going on forever. It's a really long tale, it's a really long story, and the story's never going to end. It's going to be a really really long, long, long, long, long, long, long, long, long, long, long\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[2] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 299\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m1 \u001b[31m75 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[31mBilly: Well it ends where the guy goes to his friend's house, and his friend is really upset. And his friend tells him to leave. And he goes home and he's really depressed and he's really sad about the whole situation. And he goes to his room and he sits down on his bed. And after a while, it's like three\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): output[3] \n",
      "INFO:root:Conversation.process_comment(): \t len(output) = 216\n",
      "INFO:root:Conversation.process_comment(): \t sentiment = \u001b[32m46 \u001b[31m10 \u001b[97m\n",
      "INFO:root:Conversation.process_comment(): \t \u001b[32mBilly: That is a really good question. I think I will leave it like that, because I'm not quite ready to tell the ending. I'm not quite ready to tell what happens in the end. I think it's better to leave it like that. \n",
      "Alec\u001b[97m\n",
      "INFO:root:Conversation.process_comment(): Top comment [0]\n",
      "INFO:root:Conversation.process_comment(): last comment = That was a really good story but how does it end?\n",
      "INFO:root:Conversation.process_comment(): sentiment = positive ((0.7240226, 0.22609752, 0.049879815))\n",
      "INFO:root:Conversation.process_comment():  Got a positive response, saving memory\n",
      "INFO:root:Conversation.process_comment(): runtime = 7.304145097732544\n",
      "INFO:root:Conversation.process_comment(): tokens_per_sec = 5.476342469212087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: What do you do when you meet someone you want to date?\u001b[97m \n",
      "\u001b[32mBilly: I usually just try to make a really really good impression. I would probably try to give her a little bit of a present, like maybe some flowers or like a really nice candle. I would also probably try to get her a really nice dinner or maybe even take her out to a nice restaurant. I would definitely make a really really good impression. \n",
      "Alec\u001b[97m \n",
      "\u001b[34mAlec: Tell me a long story?\u001b[97m \n",
      "\u001b[32mBilly: Yeah, I'm really into telling stories and I like long stories. I like to see how long I can make a story, and I like to tell them to people and see if they get bored or if they get excited. I like to hear what people's stories are, and I also like to make them laugh. I like people to be really funny\u001b[97m \n",
      "\u001b[32mAlec: That was a really good story but how does it end?\u001b[97m \n",
      "\u001b[32mBilly: Well, it actually ended really good. It ended with us being in love. I don't know how I ended up in a relationship with her, or how that happened, but it was really fun. It was a really fun story.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=\"That was a really good story but how does it end?\", is_speak_response=False)\n",
    "print(conversation.chat_histories[\"Alec\"].printf(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366801bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5edd91c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens_per_sec': 28.466796877548084,\n",
       " 'response_times': [7.099120855331421,\n",
       "  7.096461534500122,\n",
       "  7.077286958694458,\n",
       "  7.135305643081665,\n",
       "  7.122184753417969,\n",
       "  7.074657201766968,\n",
       "  7.080303192138672,\n",
       "  7.132595062255859,\n",
       "  7.083576917648315,\n",
       "  7.0937159061431885,\n",
       "  7.068677186965942,\n",
       "  7.101234674453735]}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "68ad079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAlec: Hello\u001b[97m \n",
      "\u001b[32mBilly: I'm really big on long stories and I'm big on stories in the abstract too. I'm really big on stories that are about stories too. I love stories that are about stories. I love telling stories that are about stories that are long, that are really long, that are really, really long too. I'm really into stories that are about stories, about people talking\u001b[97m \n",
      "\u001b[34mAlec: What comes after five?\u001b[97m \n",
      "\u001b[32mBilly: I'm really big on long, long stories, you know. I'm really big on stories, on really, really long stories. I like telling really, really long stories to people and I like listening to really, really long stories. I'm really into really long stories, that are really long. I love, I love, I love stories. Stories about stories\u001b[97m \n",
      "\u001b[34mAlec: Tell me about Toyota\u001b[97m \n",
      "\u001b[32mBilly: Well, Toyota is a really, really interesting automotive company. I like telling long stories about Toyota. Toyota is an automotive company. Toyota is really big on really, really long, really long stories. You know, really long, really long stories, really long stories\u001b[97m \n",
      "\u001b[34mAlec: Tell me about BMW\u001b[97m \n",
      "\u001b[32mBilly: BMW is a really, really big car company. They are huge on really, really long, really long stories. They love really long stories. They love long, really long stories. That's all they do. They tell really, really long long stories. Really long, really long stories, really long stories\u001b[97m \n",
      "\u001b[34mAlec: Tell me about Nissan\u001b[97m \n",
      "\u001b[32mBilly: Nissan is a really, really, long story. Really long stories, really long stories, really long stories, really long stories. They love really long stories. They really love really long stories. They really love telling really, really long stories. Really long, really long stories, really long stories, really long stories. Re\u001b[97m \n",
      "\u001b[34mAlec: How do I change my brake pads?\u001b[97m \n",
      "\u001b[34mBilly: You go to the auto-repair shop and they can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can show you how to change your brake pads. They can\u001b[97m \n",
      "\u001b[34mAlec: How do I change my oil?\u001b[97m \n",
      "\u001b[34mBilly: You go to the oil change place and they will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil. They will tell you where to change your oil\u001b[97m \n",
      "\u001b[34mAlec: Tell me about Starcraft\u001b[97m \n",
      "\u001b[32mBilly: Oh yeah. Starcraft. Starcraft. It's really interesting. I really really love Starcraft. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting. It's really interesting.\u001b[97m \n",
      "\u001b[34mAlec: What do you look for in a women?\u001b[97m \n",
      "\u001b[32mBilly: I look for someone who is really kind and caring and sweet, because I am really kind and caring and sweet. I also look for someone who is really really good looking. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. That really really good looks. \n",
      "Ale\u001b[97m \n",
      "\u001b[34mAlec: What do you do when you meet someone you want to date?\u001b[97m \n",
      "\u001b[32mBilly: I usually just try to make a really really good impression. I would probably try to give her a little bit of a present, like maybe some flowers or like a really nice candle. I would also probably try to get her a really nice dinner or maybe even take her out to a nice restaurant. I would definitely make a really really good impression. \n",
      "Alec\u001b[97m \n",
      "\u001b[34mAlec: Tell me a long story?\u001b[97m \n",
      "\u001b[32mBilly: Yeah, I'm really into telling stories and I like long stories. I like to see how long I can make a story, and I like to tell them to people and see if they get bored or if they get excited. I like to hear what people's stories are, and I also like to make them laugh. I like people to be really funny\u001b[97m \n",
      "\u001b[32mAlec: That was a really good story but how does it end?\u001b[97m \n",
      "\u001b[32mBilly: Well, it actually ended really good. It ended with us being in love. I don't know how I ended up in a relationship with her, or how that happened, but it was really fun. It was a really fun story.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\u001b[97m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(conversation.chat_histories[\"Alec\"].printf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "01c0cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"\"\n",
    "while comment != \"stop\" and False:\n",
    "    # Get user input\n",
    "    comment = input(\">> User:\")\n",
    "    response, wav, rate = conversation.process_comment(commentor=\"Alec\", comment=comment, is_speak_response=True)\n",
    "    print(conversation.chat_histories[\"Alec\"].printf(6))\n",
    "    #ipd.Audio(wav, rate=rate, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7a439342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversation.chat_histories[\"Alec\"].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b09be9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_generated = [\"Lilly\", \"Ashlee\", \"Alexandre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "00349b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "overlay         1.8T  579G  1.2T  34% /\n",
      "tmpfs            64M     0   64M   0% /dev\n",
      "shm              64M  8.0K   64M   1% /dev/shm\n",
      "/dev/nvme0n1p2  1.8T  579G  1.2T  34% /py\n",
      "tmpfs            63G   12K   63G   1% /proc/driver/nvidia\n",
      "tmpfs            13G  2.9M   13G   1% /run/nvidia-persistenced/socket\n",
      "udev             63G     0   63G   0% /dev/nvidia0\n",
      "tmpfs            63G     0   63G   0% /proc/asound\n",
      "tmpfs            63G     0   63G   0% /proc/acpi\n",
      "tmpfs            63G     0   63G   0% /proc/scsi\n",
      "tmpfs            63G     0   63G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52ffdbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot.tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567636b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d02354ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 18299, 29901, 29871],\n",
       " [1, 350, 29901, 29871],\n",
       " [1, 529, 29933, 2891, 29958],\n",
       " [1, 1533, 29933, 2891, 29958],\n",
       " [1, 529, 25826, 29958],\n",
       " [1, 5196, 29874, 29901],\n",
       " [1, 1095, 974, 726],\n",
       " [1, 529, 29989]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot.tokenizer(robot.stopping_words).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fce83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51bfd9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wav, rate = robot.read_response(\"ItzAGundam\")\n",
    "#IPython.display.display(IPython.display.Audio(wav, rate=rate * .9, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wav, rate = robot.read_response(\"Ann3thrax\")\n",
    "#IPython.display.display(IPython.display.Audio(wav, rate=rate * .9, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wav, rate = robot.read_response(\"EazyMoney\")\n",
    "#IPython.display.display(IPython.display.Audio(wav, rate=rate * .9, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece01a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#robot.model.save_pretrained(\"test_save_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47994800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from voicebox import VoiceBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5555bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice = VoiceBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec239c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158afccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, rate = voice.read_text(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac746699",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.display(IPython.display.Audio(wav, rate=rate, autoplay=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03623a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.887px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
