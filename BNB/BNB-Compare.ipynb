{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b68534",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb5fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8235c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3ccdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc44a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41275d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-deps --force-reinstall -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc89d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d770cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-deps --force-reinstall -U git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98918fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  --upgrade --no-deps --force-reinstall -U git+https://github.com/huggingface/peft.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  --upgrade --no-deps --force-reinstall -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dfd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0b5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteriaList\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5aba02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0.dev20230621'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10002db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.31.0.dev0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcacc55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25afbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43510caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible solution to missing libcudart\n",
    "#!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/conda/lib/libcudart.so\n",
    "#!ls /opt/conda/lib/python3.10/site-packages/bitsandbytes/\n",
    "#!cp /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda120.so /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b06b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model, in_gb:bool=True) -> str:\n",
    "    param_size:int = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size:int = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    if in_gb:\n",
    "        return f\"{round((param_size + buffer_size) / 1024**3, 1)} GB\"\n",
    "    else:\n",
    "        return f\"{int((param_size + buffer_size) / 1024**2)} MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba565cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_generation(model, tokenizer, prompt:str, target_token_count:int, is_use_gpu:bool=True) -> dict:\n",
    "    \"\"\"\n",
    "    Given a transformer model, evaluate how long it takes to generate output.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : transformers.models\n",
    "        A trained HuggingFace CasualLanguage model to use for generating text\n",
    "    tokenizer \n",
    "        An accompanying tokenizer used by the lanugage model\n",
    "    target_token_count : int\n",
    "        The number of tokens to generate, can be less if stopping tokens are encountered\n",
    "    is_use_gpu : bool\n",
    "        Flag whether or not to use the gpu for inference\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Containing results from the generation process including:\n",
    "        output : The generated text\n",
    "        output_count : The number of tokens in the output\n",
    "        tokens_per_sec : The number of tokens generated per second\n",
    "            \n",
    "    \"\"\"\n",
    "    if is_use_gpu:\n",
    "        tokenized_items = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    else: \n",
    "        tokenized_items = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    start = time.time()\n",
    "    logits = model.generate(\n",
    "                            min_length=target_token_count, \n",
    "                            max_length=target_token_count, \n",
    "                            do_sample=True,\n",
    "                            **tokenized_items\n",
    "                           )\n",
    "    output = tokenizer.decode(logits[0], skip_special_tokens=True)\n",
    "    runtime = time.time() - start\n",
    "    output_count = len(output.split(\" \"))\n",
    "    tokens_per_sec = output_count / runtime\n",
    "    output_count, int(runtime), int(tokens_per_sec)\n",
    "    return {\"output\" : output, \"output_count\": output_count, \"tokens_per_sec\": tokens_per_sec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "593df8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"PygmalionAI/pygmalion-2.7b\"\n",
    "prompt = '''Billy's Persona: Billy is an angry pirate lost at sea. He misses his leg.\n",
    "<START>\n",
    "You: What do you look for in a woman?\n",
    "Billy:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb6be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_run_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3acfab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b4bfeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non quantized\n",
      "Size: 10.0 GB\n",
      "Speed: 4 tps\n"
     ]
    }
   ],
   "source": [
    "if is_run_quant:\n",
    "    # Init bits and bytes config\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "       load_in_4bit=True,\n",
    "       bnb_4bit_quant_type=\"nf4\",\n",
    "       bnb_4bit_use_double_quant=True,\n",
    "       bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    results = time_generation(model_nf4, tokenizer, prompt, 1000)\n",
    "    # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    #print(json.dumps(results,indent=3))\n",
    "    tokens_per_sec = int(results[\"tokens_per_sec\"])\n",
    "    print (\"Model quantized\")\n",
    "    print (f\"Size: {get_model_size(model_nf4)}\")\n",
    "    print (f\"Speed: {tokens_per_sec} tps\")\n",
    "else:\n",
    "    # Init word tokenizer\n",
    "    full_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # Init language model\n",
    "    full_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    full_model.to(\"cuda\")\n",
    "    results = time_generation(full_model, full_tokenizer, prompt, 1000)\n",
    "    # Clear memory\n",
    "    torch.cuda.empty_cache()\n",
    "    #print(json.dumps(results,indent=3))\n",
    "    tokens_per_sec = int(results[\"tokens_per_sec\"])\n",
    "    print (\"Non quantized\")\n",
    "    print (f\"Size: {get_model_size(full_model)}\")\n",
    "    print (f\"Speed: {tokens_per_sec} tps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32050f",
   "metadata": {},
   "source": [
    "Model quantized\n",
    "\n",
    "Size: 1.5 GB\n",
    "\n",
    "Speed: 7 tps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6201d8d",
   "metadata": {},
   "source": [
    "Non quantized\n",
    "\n",
    "Size: 10.0 GB\n",
    "\n",
    "Speed: 4 tps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33150a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd96982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
